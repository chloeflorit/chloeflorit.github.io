---
  layout: post
title: HW 4
---
  
  In this blog post, I'll be creating a fake news classification model to determine whether an article is a news article is fake based on title or text or both. 

# §1. Making a dataset

We will create a function called `make_dataset` that will remove stopwords and return a `tf.data.Dataset` with 2 inputs and one output. 

```python
import nltk
nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words('english')

def make_dataset(data):
  
  pat = r'\b(?:{})\b'.format('|'.join(stopwords))
  data['title'] = data['title'].str.replace(pat, '')
  data['title'] = data['title'].str.replace(r'\s+', ' ')
  data['text'] = data['text'].str.replace(pat, '')
  data['text'] = data['text'].str.replace(r'\s+', ' ')

  data = tf.data.Dataset.from_tensor_slices(
    ( # dictionary for input data/features
        { "title": data[["title"]],
     "text": data[["text"]]
      },
    # dictionary for output data/labels
      { "fake": data[["fake"]]
        
      }   
    )
  )
  data = data.batch(100)
  return(data)

data = make_dataset(data)
```
## Validation Data
Let's use 20% of our primary Dataset for validation. 

``` python 
data = data.shuffle(buffer_size = len(data))
train_size = int(0.8*len(data))
val_size = int(0.2*len(data))

train = data.take(train_size)
val = data.skip(train_size).take(val_size)
```
## Base Rate

Let's calculate the base rate. 
Let's assume that our model always guesses "fake news" with label 1. 
 ```python
labels_iterator= train.unbatch().map(lambda text, label: label['fake']).as_numpy_iterator() #creates an iterator with the different values

numberOfOnes = 0 #variable which will count the number of times label 1 shows up
totalNumberValues = 0 #variable which will count total number of labels
for a in labels_iterators:
    if a[0]==1:
        numberOfOnes +=1
    
    totalNumberValues +=1 
baseRate = numberOfONes/totalNumberValues #base rate
 ```
The base rate is 53%. Our model will be accurate 53% of the time if it always guesses fake. 
 # §2. Creating Models
Let's create TensorFlow mdoels to determine whether when detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both? 

## First model
We will only use the article title as the input
 ```python
title_input = keras.Input(
    shape=(1,),
    name = "title", # same name as the dictionary key in the dataset
    dtype = "string"
)

title_features = title_vectorize_layer(title_input) # apply this "function TextVectorization layer" to lyrics_input
title_features = layers.Embedding(size_vocabulary, output_dim = 3, name="embedding")(title_features)
title_features = layers.Dropout(0.2)(title_features) # randomly "drop" or delete 20% of connections between the previous layer and the next layer
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features) # randomly "drop" or delete 20% of connections between the previous layer and the next layer
title_features = layers.Dense(32, activation='relu')(title_features)

output = layers.Dense(2, name="fake")(title_features)

model = keras.Model(
    inputs = [title_input],
    outputs = output
)

history = model.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    )
                  
 ```

 ![image-example.png](/images/model2LastHW.png)
## Second model
We will only use the article text as the input. 
  ```python
text_input = keras.Input(
    shape=(1,),
    name = "text", # same name as the dictionary key in the dataset
    dtype = "string"
)

text_features = title_vectorize_layer(text_input) # apply this "function TextVectorization layer" to lyrics_input
text_features = layers.Embedding(size_vocabulary, output_dim = 3, name="embedding")(text_features)#Turns positive integers (indexes) into dense vectors of size 3
text_features = layers.Dropout(0.2)(text_features) # randomly "drop" or delete 20% of connections between the previous layer and the next layer
text_features = layers.GlobalAveragePooling1D()(text_features) 
text_features = layers.Dropout(0.2)(text_features) # randomly "drop" or delete 20% of connections between the previous layer and the next layer
text_features = layers.Dense(32, activation='relu')(text_features)

output1 = layers.Dense(2, name="fake")(text_features) 

modelText = keras.Model(
    inputs = [text_input],
    outputs = output1
    )

modelText.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

history = modelText.fit(train, 
                    validation_data=val,
                    epochs = 50)

   ```


 ![image-example.png](/images/model4History.png)

## Third model
We will use the article text and the article title as the input.

```python
text_features = title_vectorize_layer(text_input) # apply this "function TextVectorization layer" to lyrics_input
text_features = layers.Dense(32, activation='relu')(text_features)

main = layers.concatenate([title_features, text_features], axis = 1)
main = layers.Dense(32, activation='relu')(main) # 32 classes in dataset
output2 = layers.Dense(2, name="fake")(main) 

modelTextTitle = keras.Model(
    inputs = [title_input, text_input],
    outputs = output2
)

modelTextTitle.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

history = modelTextTitle.fit(train, 
                    validation_data=val,
                    epochs = 20)
              
```

![image-example.png](/images/model1LastHW.png)

# §3. Model Evaluation
Now we’ll test your model performance on unseen test data.
```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
testData = pd.read_csv(test_url)
testData1 = make_dataset(testData)
model.evaluate(testData1)
```
![image-example.png](/images/resultsHigh.png)

## Embedding visualization
Let's create an embedding visualization. 
```python
weights = model.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
vocab = title_vectorize_layer.get_vocabulary() # get the vocabulary from our data prep for later

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})

#creates an embedding plot
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = [2]*len(embedding_df),
                
                 hover_name = "word")

fig.show()

```

![image-example.png](/images/embedding.png)

Interpretation of the data: The words liberal, conspiracy, trump, illegal, terrorists can be seen together on the far right of our plot. These words tend to show up often in fake news articles. 